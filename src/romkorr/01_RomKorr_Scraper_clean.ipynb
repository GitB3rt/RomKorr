{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RomKorr – Scraper (clean)\n",
        "\n",
        "Dieses Notebook lädt Metadaten zu Briefen von **briefe-der-romantik.de** und schreibt zwei Dateien:\n",
        "\n",
        "- `data/raw/rom_korr_full.csv` – alles, was gefunden wird\n",
        "- `data/processed/rom_korr_full_website.csv` – gefiltert: Zeilen, bei denen *alle* Kernfelder `Unknown` sind (bzw. Dispatch/Destination beide `Unknown`)\n",
        "\n",
        "> Hinweis: Bitte scrapen mit Maß (Rate-Limit) und im Zweifel `END_ID` klein starten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import re\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Konfiguration\n",
        "# =========================\n",
        "\n",
        "BASE_URL = \"https://briefe-der-romantik.de/letters/view/\"\n",
        "START_ID = 1\n",
        "END_ID = 21000\n",
        "\n",
        "# Politeness / Stabilität\n",
        "SLEEP_SECONDS = 0.5           # zwischen Requests\n",
        "TIMEOUT_SECONDS = 30\n",
        "MAX_RETRIES = 3\n",
        "BACKOFF_FACTOR = 1.5          # exponential backoff\n",
        "USER_AGENT = \"RomKorr/1.0 (research; contact: you@example.com)\"  # bitte anpassen\n",
        "\n",
        "# Output\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "RAW_OUT = PROJECT_ROOT / \"data\" / \"raw\" / \"rom_korr_full.csv\"\n",
        "PROCESSED_OUT = PROJECT_ROOT / \"data\" / \"processed\" / \"rom_korr_full_website.csv\"\n",
        "CACHE_DIR = PROJECT_ROOT / \"data\" / \"cache\" / \"html\"   # optional: HTML cache pro ID\n",
        "\n",
        "# Wenn True: speichert HTML unter data/cache/html/<id>.html und nutzt Cache beim Restart\n",
        "USE_HTML_CACHE = True\n",
        "\n",
        "# Wenn True: setzt die URL ohne query_id-Parameter (robuster, falls query_id wechselt)\n",
        "DROP_QUERY_ID = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Helpers\n",
        "# =========================\n",
        "\n",
        "@dataclass\n",
        "class LetterMeta:\n",
        "    letter_id: int\n",
        "    Date: Optional[str] = None\n",
        "    Sender: Optional[str] = None\n",
        "    Recipient: Optional[str] = None\n",
        "    Place_of_Dispatch: Optional[str] = None\n",
        "    Place_of_Destination: Optional[str] = None\n",
        "    Dispatch_GeoNames: Optional[str] = None\n",
        "    Destination_GeoNames: Optional[str] = None\n",
        "    link: Optional[str] = None\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return {\n",
        "            \"letter_id\": self.letter_id,\n",
        "            \"Date\": self.Date,\n",
        "            \"Sender\": self.Sender,\n",
        "            \"Recipient\": self.Recipient,\n",
        "            \"Place of Dispatch\": self.Place_of_Dispatch,\n",
        "            \"Place of Destination\": self.Place_of_Destination,\n",
        "            \"Dispatch_GeoNames\": self.Dispatch_GeoNames,\n",
        "            \"Destination_GeoNames\": self.Destination_GeoNames,\n",
        "            \"link\": self.link,\n",
        "        }\n",
        "\n",
        "\n",
        "def clean_kdata_text(text: str) -> str:\n",
        "    \\\"\\\"\\\"Nimmt nur das Segment vor dem ersten '·' (z.B. ohne '· GND').\\\"\\\"\\\"\n",
        "    return re.split(r\"\\\\s*·\\\\s*\", text, maxsplit=1)[0].strip()\n",
        "\n",
        "\n",
        "def build_letter_url(letter_id: int) -> str:\n",
        "    if DROP_QUERY_ID:\n",
        "        # bewusst minimal: query_id ist häufig instabil\n",
        "        return f\"{BASE_URL}{letter_id}?left=text\"\n",
        "    return f\"{BASE_URL}{letter_id}?left=text&query_id=6773ee556fe85\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_session() -> requests.Session:\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\n",
        "        \"User-Agent\": USER_AGENT,\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"de-DE,de;q=0.9,en;q=0.7\",\n",
        "    })\n",
        "    return s\n",
        "\n",
        "\n",
        "def fetch_html(session: requests.Session, letter_id: int) -> Optional[str]:\n",
        "    \\\"\\\"\\\"Lädt HTML (mit Retries). Optional: lokaler Cache pro letter_id.\\\"\\\"\\\"\n",
        "    url = build_letter_url(letter_id)\n",
        "    cache_file = CACHE_DIR / f\"{letter_id}.html\"\n",
        "\n",
        "    if USE_HTML_CACHE and cache_file.exists():\n",
        "        return cache_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "    last_err: Optional[Exception] = None\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            resp = session.get(url, timeout=TIMEOUT_SECONDS)\n",
        "            if resp.status_code == 404:\n",
        "                return None\n",
        "            resp.raise_for_status()\n",
        "\n",
        "            html = resp.text\n",
        "            if USE_HTML_CACHE:\n",
        "                CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "                cache_file.write_text(html, encoding=\"utf-8\")\n",
        "\n",
        "            return html\n",
        "\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            sleep = (BACKOFF_FACTOR ** (attempt - 1)) * SLEEP_SECONDS\n",
        "            time.sleep(sleep)\n",
        "\n",
        "    print(f\"[WARN] Failed letter_id={letter_id}: {last_err}\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_metadata(html: str, letter_id: int) -> Optional[LetterMeta]:\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    metadata_section = soup.find(\"div\", id=\"metadata-tab-pane\")\n",
        "    if not metadata_section:\n",
        "        return None\n",
        "\n",
        "    meta = LetterMeta(letter_id=letter_id, link=build_letter_url(letter_id))\n",
        "\n",
        "    # Erwartetes DOM: <li><span class=\"label\">...</span><span class=\"kdata\">...</span></li>\n",
        "    for li in metadata_section.find_all(\"li\"):\n",
        "        label_span = li.find(\"span\", class_=\"label\")\n",
        "        kdata_span = li.find(\"span\", class_=\"kdata\")\n",
        "        if not label_span or not kdata_span:\n",
        "            continue\n",
        "\n",
        "        label = label_span.get_text(strip=True)\n",
        "        raw = kdata_span.get_text(strip=True)\n",
        "        value = clean_kdata_text(raw) if raw else None\n",
        "\n",
        "        # Mapping auf unser Schema\n",
        "        if label == \"Date\":\n",
        "            meta.Date = value\n",
        "        elif label == \"Sender\":\n",
        "            meta.Sender = value\n",
        "        elif label == \"Recipient\":\n",
        "            meta.Recipient = value\n",
        "        elif label == \"Place of Dispatch\":\n",
        "            meta.Place_of_Dispatch = value\n",
        "            for a in kdata_span.find_all(\"a\", href=True):\n",
        "                if \"geonames.org\" in a[\"href\"]:\n",
        "                    meta.Dispatch_GeoNames = a[\"href\"]\n",
        "                    break\n",
        "        elif label == \"Place of Destination\":\n",
        "            meta.Place_of_Destination = value\n",
        "            for a in kdata_span.find_all(\"a\", href=True):\n",
        "                if \"geonames.org\" in a[\"href\"]:\n",
        "                    meta.Destination_GeoNames = a[\"href\"]\n",
        "                    break\n",
        "\n",
        "    return meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_range(start_id: int, end_id: int) -> pd.DataFrame:\n",
        "    session = get_session()\n",
        "\n",
        "    rows = []\n",
        "    for letter_id in range(start_id, end_id + 1):\n",
        "        if letter_id % 100 == 0:\n",
        "            print(f\"[INFO] at letter_id={letter_id}\")\n",
        "\n",
        "        html = fetch_html(session, letter_id)\n",
        "        if not html:\n",
        "            time.sleep(SLEEP_SECONDS)\n",
        "            continue\n",
        "\n",
        "        meta = parse_metadata(html, letter_id)\n",
        "        if meta:\n",
        "            rows.append(meta.to_dict())\n",
        "\n",
        "        time.sleep(SLEEP_SECONDS)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def filter_for_website(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Robust gegen fehlende Spalten\n",
        "    needed = [\"Sender\", \"Recipient\", \"Date\", \"Place of Dispatch\", \"Place of Destination\"]\n",
        "    for col in needed:\n",
        "        if col not in df.columns:\n",
        "            df[col] = None\n",
        "\n",
        "    mask_all_unknown = (\n",
        "        df[\"Sender\"].fillna(\"Unknown\").eq(\"Unknown\")\n",
        "        & df[\"Recipient\"].fillna(\"Unknown\").eq(\"Unknown\")\n",
        "        & df[\"Date\"].fillna(\"Unknown\").eq(\"Unknown\")\n",
        "        & df[\"Place of Dispatch\"].fillna(\"Unknown\").eq(\"Unknown\")\n",
        "        & df[\"Place of Destination\"].fillna(\"Unknown\").eq(\"Unknown\")\n",
        "    )\n",
        "\n",
        "    mask_place_unknown = (\n",
        "        df[\"Place of Dispatch\"].fillna(\"Unknown\").eq(\"Unknown\")\n",
        "        & df[\"Place of Destination\"].fillna(\"Unknown\").eq(\"Unknown\")\n",
        "    )\n",
        "\n",
        "    mask_combined = mask_all_unknown | mask_place_unknown\n",
        "    print(f\"[INFO] rows removed by filter: {int(mask_combined.sum())} / {len(df)}\")\n",
        "    return df.loc[~mask_combined].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Run\n",
        "# =========================\n",
        "\n",
        "RAW_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "PROCESSED_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df = scrape_range(START_ID, END_ID)\n",
        "\n",
        "df.to_csv(RAW_OUT, index=False)\n",
        "df_website = filter_for_website(df)\n",
        "df_website.to_csv(PROCESSED_OUT, index=False)\n",
        "\n",
        "df.head(), df_website.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "created": "2025-12-28T14:12:08.289665"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}